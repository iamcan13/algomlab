import OpenAI from 'openai';
import { InterviewTemplate, LLMResponse, CriteriaUpdate, NextQuestion } from '../../types';

export class LLMMapper {
  private openai: OpenAI;
  private model: string;

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    this.model = process.env.GPT_MODEL || 'gpt-4o-mini';
    console.log(`LLMMapper ì´ˆê¸°í™”: ëª¨ë¸=${this.model}`);
  }

  /**
   * í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ ê¸°ì¤€ ì—…ë°ì´íŠ¸ ë° ì§ˆë¬¸ ìƒì„±
   */
  async processTranscript(
    transcript: string,
    template: InterviewTemplate,
    conversationHistory: string[] = []
  ): Promise<LLMResponse | null> {
    const startTime = Date.now();
    const requestId = Math.random().toString(36).substr(2, 9);
    
    console.log(`\nğŸš€ [LLM-${requestId}] í˜¸ì¶œ ì‹œì‘`);
    console.log(`ğŸ“ [LLM-${requestId}] ì…ë ¥ í…ìŠ¤íŠ¸: "${transcript.substring(0, 100)}..."`);
    console.log(`ğŸ“Š [LLM-${requestId}] í…œí”Œë¦¿: ${template.role} (${template.criteria.length}ê°œ ê¸°ì¤€)`);
    console.log(`ğŸ§  [LLM-${requestId}] ëª¨ë¸: ${this.model}`);
    
    try {
      const prompt = this.buildPrompt(transcript, template, conversationHistory);
      console.log(`ğŸ“„ [LLM-${requestId}] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: ${prompt.length}ì`);
      
      const apiCallStart = Date.now();
      const response = await this.openai.chat.completions.create({
        model: this.model,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: 'json_object' }
      });

      const apiCallTime = Date.now() - apiCallStart;
      console.log(`â±ï¸ [LLM-${requestId}] API í˜¸ì¶œ ì‹œê°„: ${apiCallTime}ms`);
      
      // í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
      if (response.usage) {
        console.log(`ğŸ’° [LLM-${requestId}] í† í° ì‚¬ìš©ëŸ‰: ${response.usage.prompt_tokens} + ${response.usage.completion_tokens} = ${response.usage.total_tokens}`);
      }

      const content = response.choices[0]?.message?.content;
      if (!content) {
        throw new Error('OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤');
      }

      console.log(`ğŸ“¤ [LLM-${requestId}] ì‘ë‹µ ê¸¸ì´: ${content.length}ì`);
      
      const llmResponse: LLMResponse = JSON.parse(content);
      
      // ì‘ë‹µ ìœ íš¨ì„± ê²€ì¦
      if (!this.validateLLMResponse(llmResponse)) {
        throw new Error('ìœ íš¨í•˜ì§€ ì•Šì€ LLM ì‘ë‹µ í˜•ì‹');
      }

      const totalTime = Date.now() - startTime;
      console.log(`âœ… [LLM-${requestId}] ì²˜ë¦¬ ì™„ë£Œ: ${llmResponse.criteria_updates.length}ê°œ ê¸°ì¤€ ì—…ë°ì´íŠ¸, ${llmResponse.next_questions.length}ê°œ ì§ˆë¬¸ ìƒì„±`);
      console.log(`â±ï¸ [LLM-${requestId}] ì´ ì²˜ë¦¬ ì‹œê°„: ${totalTime}ms\n`);
      
      return llmResponse;

    } catch (error) {
      const totalTime = Date.now() - startTime;
      console.error(`âŒ [LLM-${requestId}] ì²˜ë¦¬ ì‹¤íŒ¨ (${totalTime}ms):`, error);
      console.error(`ğŸ” [LLM-${requestId}] ì—ëŸ¬ ìƒì„¸:`, {
        message: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
        type: error instanceof Error ? error.constructor.name : typeof error,
        stack: error instanceof Error ? error.stack?.split('\n').slice(0, 3).join('\n') : undefined
      });
      console.log(`\n`);
      return null;
    }
  }

  /**
   * í”„ë¡¬í”„íŠ¸ êµ¬ì„±
   */
  private buildPrompt(
    transcript: string,
    template: InterviewTemplate,
    conversationHistory: string[]
  ): string {
    const criteriaList = template.criteria
      .map(c => `- ${c.id}: ${c.label} (${c.rubric}) [í˜„ì¬: ${c.status}]`)
      .join('\n');

    const historyText = conversationHistory.length > 0 
      ? `\n\nì´ì „ ëŒ€í™”:\n${conversationHistory.slice(-5).join('\n')}`
      : '';

    return `
ë‹¹ì‹ ì€ ${template.role} ë©´ì ‘ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
${criteriaList}

í˜„ì¬ ë©´ì ‘ìì˜ ë‹µë³€:
"${transcript}"
${historyText}

ì§€ì‹œì‚¬í•­:
1. ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ê° í‰ê°€ ê¸°ì¤€ì— ëŒ€í•œ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”
2. ìƒíƒœ: unknown(ì •ë³´ì—†ìŒ) â†’ weak(ë¶€ì¡±í•¨) â†’ covered(ì¶©ì¡±í•¨)
3. evidenceëŠ” êµ¬ì²´ì ì´ê³  ê°ê´€ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. confidenceëŠ” 0.0-1.0 ì‚¬ì´ë¡œ í‰ê°€í•˜ì„¸ìš”
5. ë¯¸ê²€ì¦/ì•½í•œ ê¸°ì¤€ì— ëŒ€í•´ 1-2ê°œì˜ ì§ˆë¬¸ì„ ì œì•ˆí•˜ì„¸ìš”
6. ì§ˆë¬¸ì€ í•œêµ­ì–´ë¡œ ì§ì„¤ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
{
  "criteria_updates": [
    {
      "id": "ê¸°ì¤€_id",
      "status": "unknown|weak|covered",
      "evidence": ["êµ¬ì²´ì ì¸ ê·¼ê±°1", "êµ¬ì²´ì ì¸ ê·¼ê±°2"],
      "confidence": 0.85
    }
  ],
  "next_questions": [
    {
      "id": "ê¸°ì¤€_id",
      "ask": "ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸"
    }
  ]
}

ì¤‘ìš”: ë‹µë³€ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê¸°ì¤€ì€ ì—…ë°ì´íŠ¸í•˜ì§€ ë§ˆì„¸ìš”. ì¶”ì¸¡ì´ë‚˜ ê°€ì •ìœ¼ë¡œ í‰ê°€í•˜ì§€ ë§ˆì„¸ìš”.
`;
  }

  /**
   * LLM ì‘ë‹µ ìœ íš¨ì„± ê²€ì¦
   */
  private validateLLMResponse(response: any): response is LLMResponse {
    if (!response || typeof response !== 'object') {
      return false;
    }

    // criteria_updates ê²€ì¦
    if (!Array.isArray(response.criteria_updates)) {
      return false;
    }

    for (const update of response.criteria_updates) {
      if (!update.id || !update.status || !Array.isArray(update.evidence)) {
        return false;
      }
      if (!['unknown', 'weak', 'covered'].includes(update.status)) {
        return false;
      }
      if (typeof update.confidence !== 'number' || update.confidence < 0 || update.confidence > 1) {
        return false;
      }
    }

    // next_questions ê²€ì¦
    if (!Array.isArray(response.next_questions)) {
      return false;
    }

    for (const question of response.next_questions) {
      if (!question.id || !question.ask) {
        return false;
      }
    }

    return true;
  }

  /**
   * ê±´ê°•ì„± ì²´í¬
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await this.openai.chat.completions.create({
        model: this.model,
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5
      });
      
      return !!response.choices[0]?.message?.content;
    } catch (error) {
      console.error('LLM ê±´ê°•ì„± ì²´í¬ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * í†µê³„ ì •ë³´ ë°˜í™˜
   */
  getStats() {
    return {
      model: this.model,
      hasApiKey: !!process.env.OPENAI_API_KEY
    };
  }
}